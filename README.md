# ml-mini-lib

Mini biblioteka Machine Learning napisana w Pythonie, implementujÄ…ca
prostÄ… sieÄ‡ neuronowÄ… (feedforward) z **rÄ™cznie zaimplementowanym backpropagation**  
oraz **opcjonalnym przyspieszeniem GPU (CUDA / CuPy)**.

Projekt zostaÅ‚ stworzony w celach edukacyjnych â€“ do nauki:
- dziaÅ‚ania sieci neuronowych â€od zeraâ€
- rÃ³Å¼nic miÄ™dzy CPU i GPU
- implementacji podstawowych algorytmÃ³w optymalizacji

---

## âœ¨ FunkcjonalnoÅ›ci

- âœ… SieÄ‡ neuronowa typu **2 â†’ hidden â†’ 1**
- âœ… RÄ™czny **forward pass** i **backpropagation**
- âœ… Aktywacje: `tanh`, `sigmoid`
- âœ… Funkcja straty: **MSE**
- âœ… Optymalizatory:
  - SGD
  - SGD z momentum
  - Adam
- âœ… Backend:
  - **CPU (NumPy)** â€“ zawsze dostÄ™pny
  - **GPU (CuPy + CUDA)** â€“ opcjonalny
- âœ… Wizualizacja:
  - wykres strat (loss)
  - granica decyzyjna XOR
- âœ… Test numeryczny gradientÃ³w
- âœ… Zapis i odczyt wag modelu

---

## ğŸ§  PrzykÅ‚ad problemu â€“ XOR

Biblioteka demonstruje rozwiÄ…zanie klasycznego problemu XOR:

| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 1 |
| 1  | 0  | 1 |
| 1  | 1  | 0 |

XOR **nie jest liniowo separowalny**, dlatego wymaga warstwy ukrytej.

---

## ğŸ“ Struktura projektu

ml-mini-lib/

â”‚

â”œâ”€â”€ mllib/ # biblioteka ML

â”‚ â”œâ”€â”€ backend.py

â”‚ â”œâ”€â”€ tensor_ops.py

â”‚ â”œâ”€â”€ layers.py

â”‚ â”œâ”€â”€ model.py

â”‚ â”œâ”€â”€ optimizers.py

â”‚ â”œâ”€â”€ training.py

â”‚ â”œâ”€â”€ utils.py

â”‚ â””â”€â”€ viz.py

â”‚

â”œâ”€â”€ apps/

â”‚ â”œâ”€â”€ xor_demo/ # demo XOR

â”‚ â”œâ”€â”€ benchmark/ # CPU vs GPU

â”‚ â””â”€â”€ playground/ # eksperymenty

â”‚

â”œâ”€â”€ setup.py

â”œâ”€â”€ pyproject.toml

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md


yaml
Skopiuj kod

---

## âš™ï¸ Wymagania

### Podstawowe (CPU)
- Python â‰¥ 3.9
- NumPy
- Matplotlib

### Opcjonalne (GPU)
- NVIDIA GPU
- CUDA
- CuPy (`cupy-cuda12x`)

---

## ğŸš€ Instalacja (zalecane: virtualenv)

### 1ï¸âƒ£ Utworzenie i aktywacja venv

```bash
python -m venv .venv
.venv\Scripts\Activate.ps1
2ï¸âƒ£ Instalacja zaleÅ¼noÅ›ci
bash
Skopiuj kod
pip install -r requirements.txt
3ï¸âƒ£ Instalacja biblioteki (tryb developerski)
bash
Skopiuj kod
pip install -e .
â–¶ï¸ Uruchomienie demo XOR
bash
Skopiuj kod
python apps/xor_demo/main.py
Po treningu model powinien poprawnie klasyfikowaÄ‡ XOR:

css
Skopiuj kod
Input  ->  Prediction
[0, 0] -> 0
[0, 1] -> 1
[1, 0] -> 1
[1, 1] -> 0
ğŸ§ª CPU vs GPU
Backend wybierany jest jawnie:

python
Skopiuj kod
net = SimpleFFN(2, 8, 1, backend="cpu")  # NumPy
net = SimpleFFN(2, 8, 1, backend="gpu")  # CuPy (jeÅ›li dostÄ™pne)
JeÅ›li GPU/CUDA nie jest dostÄ™pne, biblioteka automatycznie dziaÅ‚a na CPU.

ğŸ“Š Wizualizacje
Wykres spadku funkcji straty (loss vs epoch)

Granica decyzyjna wyuczona przez sieÄ‡ neuronowÄ…

ğŸ§ª Test gradientÃ³w
Biblioteka zawiera numeryczne sprawdzanie gradientÃ³w w celu
weryfikacji poprawnoÅ›ci backpropagation.

ğŸ“Œ Cel projektu
Projekt ma charakter edukacyjny i sÅ‚uÅ¼y do:

nauki ML â€od podstawâ€

zrozumienia matematyki sieci neuronowych

porÃ³wnania CPU vs GPU

przygotowania pod dalsze rozszerzenia (CNN, Softmax, Cross-Entropy)

ğŸ§© MoÅ¼liwe rozszerzenia

Softmax + CrossEntropy

Batch Normalization (peÅ‚na wersja)

Convolutional layers

Autograd

Eksport do ONNX
